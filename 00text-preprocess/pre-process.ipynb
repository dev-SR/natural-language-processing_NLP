{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Text Pre-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cd .\\00text-preprocess\\\n",
    "jupyter nbconvert --to markdown pre-process.ipynb --output README.md\n",
    "\"\"\"\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is an essential step in building a Machine Learning model and depending on how well the data has been preprocessed; the results are seen.\n",
    "\n",
    "In NLP, text preprocessing is the first step in the process of building a model.\n",
    "The various text preprocessing steps are:\n",
    "\n",
    "- Tokenization\n",
    "- Lower casing\n",
    "- Stop words removal\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bags of Word Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the Data/Corpus \n",
    "- Tokenization,Stop words removal\n",
    "- Stemming,Lemmatization\n",
    "- Building a Vocab\n",
    "- Vectorization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data/Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())\n",
    "print(len(brown.categories()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most corpora consist of a set of files, each containing a document (or other pieces of text). A list of identifiers for these files is accessed via the `fileids()` method of the corpus reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20']\n"
     ]
    }
   ],
   "source": [
    "print(brown.fileids()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Thirty-three'], ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.'], ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = brown.sents(categories=\"fiction\")\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scotty did not go back to school .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: Splitting the sentence into words.\n",
    "Strings can be tokenized into tokens via `nltk.word_tokenize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "# prerequisite:nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Does this thing really work? Lets see.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does this thing really work?', 'Lets see.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does', 'this', 'thing', 'really', 'work', '?', 'Lets', 'see', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sample_text)\n",
    "words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words removal: Stop words are very commonly used words (**a, an, the, etc.**) in the documents. These words do not really signify any importance as they do not help in distinguishing two documents. We can use `nltk.corpus.stopwords.words(‘english’)` to fetch a list of `stopwords` in the English dictionary. Then, we remove the tokens that are `stopwords`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "print(stop[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does', 'this', 'thing', 'really', 'work', '?', 'Lets', 'see', '.']\n",
      "['Does', 'thing', 'really', 'work', '?', 'Lets', 'see', '.']\n"
     ]
    }
   ],
   "source": [
    "clean_words = [w for w in words if w not in stop ]\n",
    "print(words)\n",
    "print(clean_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> !! **Watch out for Uppercase**: for example `this` in the above got removed as it is a stopword. But if we would have used `This`, it will not be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does', 'This', 'thing', 'really', 'work', '?', 'Lets', 'see', '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Does This thing really work? Lets see.\"\n",
    "words = word_tokenize(sample_text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Does', 'This', 'thing', 'really', 'work', '?', 'Lets', 'see', '.']\n",
      "['Does', 'This', 'thing', 'really', 'work', '?', 'Lets', 'see', '.']\n"
     ]
    }
   ],
   "source": [
    "clean_words = [w for w in words if w not in stop]\n",
    "print(words)\n",
    "print(clean_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Solution:\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does', 'this', 'thing', 'really', 'work', '?', 'lets', 'see', '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Does This thing really work? Lets see.\"\n",
    "sample_text = sample_text.lower()\n",
    "words = word_tokenize(sample_text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['does', 'this', 'thing', 'really', 'work', '?', 'lets', 'see', '.']\n",
      "['thing', 'really', 'work', '?', 'lets', 'see', '.']\n"
     ]
    }
   ],
   "source": [
    "clean_words = [w for w in words if w not in stop]\n",
    "print(words)\n",
    "print(clean_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### including punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "stop = stop + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['does', 'this', 'thing', 'really', 'work', '?', 'lets', 'see', '.']\n",
      "['thing', 'really', 'work', 'lets', 'see']\n"
     ]
    }
   ],
   "source": [
    "clean_words = [w for w in words if w not in stop]\n",
    "print(words)\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "stops = stopwords.words('english')\n",
    "stops = stops + punctuations\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\tuseful_words = [w for w in text if w not in stops]\n",
    "\treturn useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing', 'really', 'work', 'lets', 'see']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Does This thing really work? Lets see.\"\n",
    "sample_text = sample_text.lower()\n",
    "words = word_tokenize(sample_text)\n",
    "usefull_text = remove_stopwords(words)\n",
    "usefull_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does', 'thing', 'really', 'work?', 'Lets', 'see.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = \"Does this thing really work? Lets see.\".split()\n",
    "usefull_text = remove_stopwords(words)\n",
    "usefull_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Send all the documents related to chapters at prateek@cb com'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateek@cb.com\"\n",
    "# include all words(caputare @ also), exclude numbers \n",
    "tokenizer = RegexpTokenizer('[a-zA-Z@]+') \n",
    "useful_text = tokenizer.tokenize(sentence)\n",
    "\" \".join(useful_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Stemming: It is a process of transforming a word to its root form.\n",
    "\n",
    "- Process that transforms particular words(verbs,plurals)into their radical form\n",
    "- Preserve the semantics of the sentence without increasing the number of unique tokens\n",
    "- • Example - `jumps, jumping, jumped, jump` ==> `jump`\n",
    "\n",
    "We stem the tokens using `nltk.stem.porter.PorterStemmer`  to get the stemmed tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['play', 'play', 'player', 'play']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"play\", \"playing\", \"player\", \"played\"]\n",
    "\n",
    "stemmed_words = [porter.stem(w) for w in words]\n",
    "stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machin', 'happi']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"machine\",\"happying\"]\n",
    "\n",
    "stemmed_words = [ps.stem(w) for w in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The word `'machine'` has its suffix `'e'` chopped off. The stem does not make sense as it is not a word in English. This is a disadvantage of stemming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SnowballStemmer` and `LancasterStemmer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   \n",
      "friend              friend              friend              \n",
      "friendship          friendship          friend              \n",
      "friends             friend              friend              \n",
      "friendships         friendship          friend              \n",
      "stabil              stabil              stabl               \n",
      "destabilize         destabil            dest                \n",
      "misunderstanding    misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\", \"stabil\",\n",
    "             \"destabilize\", \"misunderstanding\", \"railroad\", \"moonlight\", \"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\n",
    "    \"Word\", \"Porter Stemmer\", \"lancaster Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(\n",
    "        word, porter.stem(word), lancaster.stem(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` introduced `SnowballStemmers` that are used to create `non-English`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generous'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "englishStemmer.stem(\"generously\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ‘english’ stemmer is better than the original ‘porter’ stemmer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gener'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem(\"generously\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS(part of speech) tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `nltk.pos_tag` to retrieve the `part of speech` of each token in a list.\n",
    "\n",
    "pos_tag **abbreviations**:\n",
    "\n",
    "- NNP:\t\tproper noun, singular (sarah)\n",
    "- NNS: noun, common, plural\n",
    "- NNPS:\t\tproper noun, plural (indians or americans)\n",
    "- PDT:\t\tpredeterminer (all, both, half)\n",
    "- POS:\t\tpossessive ending (parent\\ ‘s)\n",
    "- DT: determiner\n",
    "- .....\n",
    "\n",
    "[https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk](https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('any', 'DT')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tag(['any'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Watch Out: post_tag takes `list` not  `string`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "# Prerequisite: \n",
    "# nltk.download('state_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1945-Truman.txt', '1946-Truman.txt', '1947-Truman.txt', '1948-Truman.txt', '1949-Truman.txt', '1950-Truman.txt', '1951-Truman.txt', '1953-Eisenhower.txt', '1954-Eisenhower.txt', '1955-Eisenhower.txt', '1956-Eisenhower.txt', '1957-Eisenhower.txt', '1958-Eisenhower.txt', '1959-Eisenhower.txt', '1960-Eisenhower.txt', '1961-Kennedy.txt', '1962-Kennedy.txt', '1963-Johnson.txt', '1963-Kennedy.txt', '1964-Johnson.txt']\n"
     ]
    }
   ],
   "source": [
    "documents = nltk.corpus.state_union.fileids()\n",
    "print(documents[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all. Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream. Tonight we are comforted by the hope of a glad reunion with the hus\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech  = state_union.raw('2006-GWBush.txt')\n",
    "speech[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PRESIDENT', 'NNP'),\n",
       " ('GEORGE', 'NNP'),\n",
       " ('W.', 'NNP'),\n",
       " ('BUSH', 'NNP'),\n",
       " (\"'S\", 'POS'),\n",
       " ('ADDRESS', 'NNP'),\n",
       " ('BEFORE', 'IN'),\n",
       " ('A', 'NNP'),\n",
       " ('JOINT', 'NNP'),\n",
       " ('SESSION', 'NNP')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_in_words = word_tokenize(speech)\n",
    "pos = pos_tag(speech_in_words)\n",
    "pos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**: Unlike stemming, lemmatization reduces the words to a word existing in the language.\n",
    "\n",
    "For lemmatization to resolve a word to its `lemma`, **`part of speech` of the word is required**. This helps in transforming the word into a proper root form. However, for doing so, it requires extra computational linguistics power such as a **part of speech tagger**.\n",
    "\n",
    "[what-is-the-difference-between-stemming-and-lemmatization/](https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/)\n",
    "\n",
    "Lemmatization is preferred over Stemming because lemmatization does a morphological analysis of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bat'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"bats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
      "The striped bat are hanging on their foot for best\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it didn’t do a good job. Because, `‘are’` is not converted to` ‘be’` and `‘hanging’` is not converted to `‘hang’` as expected. This can be corrected if we provide the correct **‘part-of-speech’ tag (`POS` tag)** as the second argument to `lemmatize()`. Sometimes, the same word can have a multiple lemmas based on the meaning / context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'painting'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"painting\", pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paint'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"painting\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hang'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"hanging\", pos='v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"are\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"is\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hanging', 'VBG')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"hanging\"\n",
    "postag = pos_tag([w])\n",
    "postag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Function to convert `pos_tag` abbreviations to simple form that the `lemmatize()` function takes. For example `NN`,`NNS` etc to (`n`), `VBG` etc to `v`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(tag):\n",
    "\t\n",
    "\tif tag.startswith(\"J\"):\n",
    "\t\treturn wordnet.ADJ\n",
    "\telif tag.startswith(\"V\"):\n",
    "\t\treturn wordnet.VERB\n",
    "\telif tag.startswith(\"N\"):\n",
    "\t\treturn wordnet.NOUN\n",
    "\telif tag.startswith(\"R\"):\n",
    "\t\treturn wordnet.ADV\n",
    "\telse:\n",
    "\t\treturn wordnet.NOUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hanging', 'VBG')]\n",
      "VBG --> v\n"
     ]
    }
   ],
   "source": [
    "w=\"hanging\"\n",
    "postag = pos_tag([w])\n",
    "print(postag)\n",
    "print(postag[0][1]+\" --> \",end=\"\" )\n",
    "pos = get_simple_pos(postag[0][1])\n",
    "print(pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "o = []\n",
    "for w in word_list:\n",
    "\tpostag = pos_tag([w])\n",
    "\tpos = get_simple_pos(postag[0][1])\n",
    "\tclean_word = lemmatizer.lemmatize(w, pos=pos)\n",
    "\to.append(clean_word)\n",
    "\n",
    "print(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[https://www.machinelearningplus.com/nlp/lemmatization-examples-python/](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Vocab & Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorization (AKA One-Hot Encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic ways we can numerically represent words is through the one-hot encoding method (also sometimes called count vectorizing).\n",
    "\n",
    "The idea is super simple. \n",
    "- **Create a vector that has as many dimensions as your corpora has unique words**. \n",
    "- Each unique word has a unique dimension and will be represented by a `1` in that dimension with `0s` everywhere else.\n",
    "\n",
    "The result of this? Really huge and **sparse vectors** that capture absolutely no relational information. It could be useful if you have no other option. But we do have other options, if we need that semantic relationship information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"img/count-v.jpg\" alt=\"count-v.jpg\" width=\"800px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "\t\"This is good\",\n",
    "\t\"This is bad\",\n",
    "\t\"awesome This is awesome\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      "{'this': 4, 'is': 3, 'good': 2, 'bad': 1, 'awesome': 0}\n",
      "dict_keys(['this', 'is', 'good', 'bad', 'awesome'])\n"
     ]
    }
   ],
   "source": [
    "# Now, we can inspect how our vectorizer vectorized the text\n",
    "# This will print out a list of words used, and their index in the vectors\n",
    "print('Vocabulary: ')\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we would like to actually create a vector, we can do so by passing the\n",
    "# text into the vectorizer to get back counts\n",
    "vector = vectorizer.transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full vector: \n",
      "[[0 0 1 1 1]\n",
      " [0 1 0 1 1]\n",
      " [2 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Our final vector:\n",
    "print('Full vector: ')\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fit + Transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [2, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus = vectorizer.fit_transform(corpus).toarray()\n",
    "vectorized_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reverse Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['good', 'is', 'this'], dtype='<U7'),\n",
       " array(['bad', 'is', 'this'], dtype='<U7'),\n",
       " array(['awesome', 'is', 'this'], dtype='<U7')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`N-gram `can be defined as the **contiguous sequence of n items** from a given sample of text or speech. The items can be letters, words, or base pairs according to the application. The N-grams typically are collected from a text or speech corpus (A long text dataset). An `N-gram` of size `1` is referred to as a `“unigram“`, size `2` is a `“bigram”`, size `3` is a `“trigram”`.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/n_gram_ex.png\" alt=\"n_gram_ex.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "> Why N-gram?\n",
    "\n",
    "\n",
    "An N-gram plays important role in text analysis in Machine Learning. Sometimes a **single word alone** isn’t sufficient to observe the *context* of a text. Let’s see how N-gram will useful for text analysis using an example.\n",
    "\n",
    "For example, we need to predict the sentiment of the text such as positive or negative.\n",
    "\n",
    "`text = “The Margherita pizza is not bad taste”`\n",
    "\n",
    "If we consider `unigram` or a single word for text analysis, the negative word `“bad”` lead to the wrong prediction of the text. But if we use `bigram`,  the bigram word `“not bad”` helps to predict the text as a positive sentiment.\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"img/n-gram_ex1.png\" alt=\"n_gram_ex1.png\" width=\"700px\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "\t\"This is good movie\",\n",
    "\t\"This is good movie but actor is not present\",\n",
    "\t\"this is not good movie\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8cdb09b0bc1c395d296938b19fe7764d972a7ceeffba4d3ad7ff6a3771581719"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('ProgramData': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
